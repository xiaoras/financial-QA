{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9843215a-660b-432b-b412-6f47c6da275d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9607e930",
   "metadata": {},
   "source": [
    "Download the data `train.json` and place it in the \"data folder\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d77d1dba-9518-4917-8817-3208da044af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "train_file = os.path.join(\"..\", \"data\", \"train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8fb53a3-0b5d-48ec-ab96-3d383b6aa7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(train_file) as f:\n",
    "    train_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4b6fe2e-e5ab-4de5-848b-053a6e493adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3037"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e706ced4-04fe-45b1-b70c-5002e31df16f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Prepare the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f3b71bf-787a-436b-8b51-1509a96b8859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "for datapoint in train_data:\n",
    "    keys = [key for key in datapoint.keys() if \"qa\" in key]\n",
    "    for key in keys:\n",
    "        row = {}\n",
    "        row[\"pre_text\"] = \" \".join(datapoint['pre_text'])\n",
    "        row[\"table\"] = str(datapoint['table_ori'])\n",
    "        row[\"post_text\"] = \" \".join(datapoint['post_text'])\n",
    "        row[\"question\"] = datapoint[key]['question']\n",
    "        row[\"answer\"] = datapoint[key]['answer']\n",
    "        rows.append(row)\n",
    "\n",
    "df_train = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79f5cd8b-5055-465f-a464-9104624b3478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3965"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408d47f1-64f1-45a7-ad39-65460601d181",
   "metadata": {},
   "source": [
    "Since the set is quite large, we select a random sample from it with which we will play. This should keep times and costs under control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ad994f8-3c4a-4037-a8c1-5bf73e586644",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 99\n",
    "df_sample = df_train.sample(n=100, random_state=seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d8a50e-221a-4e24-90fa-4e785c3e5d19",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Testing Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494a03ac-44fd-4f7b-9043-5d15d886fbd2",
   "metadata": {},
   "source": [
    "To evaluate a model's output, we will run it against all entries of `df_sample`, getting the output for each row, and comparing the prediciton with the `answer` column (after some cleaning, such as removing characters such as \"%\" and rounding the numbers to 1 decimal place). We create a column `score`, which is $1$ if output and answer coincide, and 0 otherwise. Finally, we will simply compute the average of the the `score` column, to get the % of questions that were answered correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd48c7b7-93d5-4688-b2b3-6a2575f6562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class TestingFramework:\n",
    "\n",
    "    def __init__(self, data, model):\n",
    "        self.data = data.copy()\n",
    "        self.model = model\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.data[['row_output', 'time', 'cost']] = self.data.apply(self.model.predict, axis=1, result_type='expand')\n",
    "        self.data[\"clean_output\"] = self.data[\"row_output\"].apply(self.extract_float_from_string)\n",
    "        self.data[\"clean_answer\"] = self.data[\"answer\"].apply(self.extract_float_from_string)\n",
    "        \n",
    "        self.data[\"score\"] = np.where(self.data[\"clean_output\"] == self.data[\"clean_answer\"], 1, 0)\n",
    "\n",
    "    # UTILITIES\n",
    "\n",
    "    def extract_float_from_string(self, input_string):\n",
    "        float_pattern = r'[-+]?\\d*\\.\\d+|\\d+'\n",
    "        matches = re.findall(float_pattern, input_string)\n",
    "        floats = [float(match) for match in matches]\n",
    "        if len(floats) > 0:\n",
    "            return np.around(floats[0], 1)\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    def get_scores(self):\n",
    "        score_statistics = self.data[\"score\"].describe()\n",
    "        time_statistics = self.data[\"time\"].describe()\n",
    "        cost_statistics = self.data[\"cost\"].describe()\n",
    "\n",
    "        statistics = pd.concat([score_statistics, time_statistics, cost_statistics], axis=1)\n",
    "        return statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9a7794-3053-4b53-8e1d-933ae58d7ed0",
   "metadata": {},
   "source": [
    "Note that the `evaluate` function relies on the model having a method `predict` that, given a row of the dataframe, returns the output, the time it took to compute it, and the cost (for non-open-source models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db24b37-4228-4cdd-9d8e-9addb6bc9713",
   "metadata": {},
   "source": [
    "Let's test thel class with a couple of \"trivial\" models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97d5dc09-415c-4e48-957f-939e60e03b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trivial:\n",
    "\n",
    "    def __init__(self, name, desired_score):\n",
    "        self.name = name\n",
    "        self.desired_score = desired_score\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "        coin_toss = np.random.choice([0, 1], size=1, p=[1-self.desired_score, self.desired_score])\n",
    "        if coin_toss == 1:\n",
    "            output = x[\"answer\"]\n",
    "        else:\n",
    "            output = \"\"\n",
    "            \n",
    "        end_time = time.time()\n",
    "        \n",
    "        elapsed_time = end_time - start_time\n",
    "        cost = 0\n",
    "\n",
    "        return output, elapsed_time, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58763918-c4bc-4513-af1d-b70ca3b25389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>time</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       score        time   cost\n",
       "count  100.0  100.000000  100.0\n",
       "mean     1.0    0.000053    0.0\n",
       "std      0.0    0.000049    0.0\n",
       "min      1.0    0.000012    0.0\n",
       "25%      1.0    0.000040    0.0\n",
       "50%      1.0    0.000043    0.0\n",
       "75%      1.0    0.000053    0.0\n",
       "max      1.0    0.000443    0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cheater = Trivial(\"cheater\", 1)\n",
    "testing_cheater = TestingFramework(df_sample, cheater)\n",
    "\n",
    "testing_cheater.evaluate()\n",
    "testing_cheater.get_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f2ab7f1-9587-4e64-ad40-19fd5b965aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>time</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.502117</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            score        time   cost\n",
       "count  100.000000  100.000000  100.0\n",
       "mean     0.480000    0.000043    0.0\n",
       "std      0.502117    0.000030    0.0\n",
       "min      0.000000    0.000030    0.0\n",
       "25%      0.000000    0.000034    0.0\n",
       "50%      0.000000    0.000037    0.0\n",
       "75%      1.000000    0.000041    0.0\n",
       "max      1.000000    0.000306    0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiftyfifty = Trivial(\"fifty-fifty\", 0.5)\n",
    "testing_fiftyfifty = TestingFramework(df_sample, fiftyfifty)\n",
    "\n",
    "testing_fiftyfifty.evaluate()\n",
    "testing_fiftyfifty.get_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2484f915-d753-4b38-9665-3f506cc73760",
   "metadata": {},
   "source": [
    "Ok, it works well. We can now use the framework to test actual models!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7617c580-581c-4984-92bd-c9facfa20b76",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Llama2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367d437d-ff20-4b7f-a66d-5b54c48929d4",
   "metadata": {},
   "source": [
    "To get a free baseline, let's start with an open source model, such as Llama2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fceed86e-7134-4107-a4df-8b5e307a2a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "153646b4-7a59-45aa-9ce3-a3d558beb9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class llama2LLM:\n",
    "\n",
    "    def __init__(self, name, model):\n",
    "        self.name = name\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, x):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        output, cost = self.create_chat_completion(x)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        return output, elapsed_time, cost\n",
    "\n",
    "    # UTILITIES\n",
    "\n",
    "    def create_chat_completion(self, x):\n",
    "\n",
    "        instructions = \"Answer the user's question returning only the numerical value: do not provide any textual comments.\"\n",
    "            \n",
    "        question = f\"QUESTION: {x['question']}\"\n",
    "        context = f\"INFORMATION:\\n'''\\n{x['table']}\\n'''\"\n",
    "\n",
    "        query = question + \"\\n\\n\" + context\n",
    "\n",
    "        try:\n",
    "            \n",
    "            response = self.model.create_chat_completion(\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": instructions\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": query\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            output = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "            cost = 0\n",
    "\n",
    "            return output, cost\n",
    "        \n",
    "        except:\n",
    "\n",
    "            return \"\", 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eb8134-8c21-412d-a431-886c69ca0fd7",
   "metadata": {},
   "source": [
    "A model that can run locally in a reasonable amount of time is 7b-Q2. You can download it from [https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/blob/main/llama-2-7b-chat.Q2_K.gguf]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6405390e-26d6-42bb-8a48-eeb81c8e3fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../models/llama2/llama-2-7b-chat.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q2_K\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 2.63 GiB (3.35 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: system memory used  = 2694.43 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 73.69 MiB\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      41.41 ms /   188 runs   (    0.22 ms per token,  4540.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =   33122.79 ms /   267 tokens (  124.06 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:        eval time =   31188.82 ms /   187 runs   (  166.79 ms per token,     6.00 tokens per second)\n",
      "llama_print_timings:       total time =   64743.48 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      18.98 ms /    97 runs   (    0.20 ms per token,  5110.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =   55253.30 ms /   373 tokens (  148.13 ms per token,     6.75 tokens per second)\n",
      "llama_print_timings:        eval time =   14505.10 ms /    96 runs   (  151.09 ms per token,     6.62 tokens per second)\n",
      "llama_print_timings:       total time =   69910.93 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      11.50 ms /    59 runs   (    0.19 ms per token,  5130.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19435.78 ms /   138 tokens (  140.84 ms per token,     7.10 tokens per second)\n",
      "llama_print_timings:        eval time =    8604.79 ms /    58 runs   (  148.36 ms per token,     6.74 tokens per second)\n",
      "llama_print_timings:       total time =   28130.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       9.63 ms /    51 runs   (    0.19 ms per token,  5295.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =   28693.88 ms /   200 tokens (  143.47 ms per token,     6.97 tokens per second)\n",
      "llama_print_timings:        eval time =    7433.29 ms /    50 runs   (  148.67 ms per token,     6.73 tokens per second)\n",
      "llama_print_timings:       total time =   36204.34 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      11.07 ms /    50 runs   (    0.22 ms per token,  4515.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =   57148.58 ms /   330 tokens (  173.18 ms per token,     5.77 tokens per second)\n",
      "llama_print_timings:        eval time =    8639.01 ms /    49 runs   (  176.31 ms per token,     5.67 tokens per second)\n",
      "llama_print_timings:       total time =   65871.71 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       8.25 ms /    44 runs   (    0.19 ms per token,  5333.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =   36664.95 ms /   266 tokens (  137.84 ms per token,     7.25 tokens per second)\n",
      "llama_print_timings:        eval time =    6277.59 ms /    43 runs   (  145.99 ms per token,     6.85 tokens per second)\n",
      "llama_print_timings:       total time =   43005.45 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       6.49 ms /    24 runs   (    0.27 ms per token,  3699.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =   42071.90 ms /   235 tokens (  179.03 ms per token,     5.59 tokens per second)\n",
      "llama_print_timings:        eval time =    4902.26 ms /    23 runs   (  213.14 ms per token,     4.69 tokens per second)\n",
      "llama_print_timings:       total time =   47023.43 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       5.27 ms /    31 runs   (    0.17 ms per token,  5879.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17795.19 ms /   110 tokens (  161.77 ms per token,     6.18 tokens per second)\n",
      "llama_print_timings:        eval time =    4003.88 ms /    30 runs   (  133.46 ms per token,     7.49 tokens per second)\n",
      "llama_print_timings:       total time =   21843.83 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       9.92 ms /    53 runs   (    0.19 ms per token,  5344.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =   20799.99 ms /   158 tokens (  131.65 ms per token,     7.60 tokens per second)\n",
      "llama_print_timings:        eval time =    7651.74 ms /    52 runs   (  147.15 ms per token,     6.80 tokens per second)\n",
      "llama_print_timings:       total time =   28529.98 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      10.11 ms /    38 runs   (    0.27 ms per token,  3760.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =   30664.02 ms /   172 tokens (  178.28 ms per token,     5.61 tokens per second)\n",
      "llama_print_timings:        eval time =    7882.59 ms /    37 runs   (  213.04 ms per token,     4.69 tokens per second)\n",
      "llama_print_timings:       total time =   38632.80 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      11.93 ms /    64 runs   (    0.19 ms per token,  5364.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   44442.43 ms /   279 tokens (  159.29 ms per token,     6.28 tokens per second)\n",
      "llama_print_timings:        eval time =    9131.63 ms /    63 runs   (  144.95 ms per token,     6.90 tokens per second)\n",
      "llama_print_timings:       total time =   53668.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      10.94 ms /    41 runs   (    0.27 ms per token,  3747.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =   25582.21 ms /   158 tokens (  161.91 ms per token,     6.18 tokens per second)\n",
      "llama_print_timings:        eval time =    8600.95 ms /    40 runs   (  215.02 ms per token,     4.65 tokens per second)\n",
      "llama_print_timings:       total time =   34267.40 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      25.16 ms /   107 runs   (    0.24 ms per token,  4252.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =   31004.26 ms /   174 tokens (  178.19 ms per token,     5.61 tokens per second)\n",
      "llama_print_timings:        eval time =   19332.33 ms /   106 runs   (  182.38 ms per token,     5.48 tokens per second)\n",
      "llama_print_timings:       total time =   50533.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      10.02 ms /    56 runs   (    0.18 ms per token,  5591.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =   22353.47 ms /   166 tokens (  134.66 ms per token,     7.43 tokens per second)\n",
      "llama_print_timings:        eval time =    7909.06 ms /    55 runs   (  143.80 ms per token,     6.95 tokens per second)\n",
      "llama_print_timings:       total time =   30343.05 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       2.10 ms /     8 runs   (    0.26 ms per token,  3811.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =   29092.77 ms /   166 tokens (  175.26 ms per token,     5.71 tokens per second)\n",
      "llama_print_timings:        eval time =    1451.70 ms /     7 runs   (  207.39 ms per token,     4.82 tokens per second)\n",
      "llama_print_timings:       total time =   30559.64 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       6.28 ms /    24 runs   (    0.26 ms per token,  3823.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15448.05 ms /    89 tokens (  173.57 ms per token,     5.76 tokens per second)\n",
      "llama_print_timings:        eval time =    4784.31 ms /    23 runs   (  208.01 ms per token,     4.81 tokens per second)\n",
      "llama_print_timings:       total time =   20280.33 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      24.31 ms /   112 runs   (    0.22 ms per token,  4606.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =   43414.24 ms /   286 tokens (  151.80 ms per token,     6.59 tokens per second)\n",
      "llama_print_timings:        eval time =   18739.40 ms /   111 runs   (  168.82 ms per token,     5.92 tokens per second)\n",
      "llama_print_timings:       total time =   62351.91 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      10.79 ms /    38 runs   (    0.28 ms per token,  3522.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =   38860.79 ms /   220 tokens (  176.64 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:        eval time =    7820.69 ms /    37 runs   (  211.37 ms per token,     4.73 tokens per second)\n",
      "llama_print_timings:       total time =   46760.34 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      28.48 ms /   121 runs   (    0.24 ms per token,  4248.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =   38835.84 ms /   267 tokens (  145.45 ms per token,     6.88 tokens per second)\n",
      "llama_print_timings:        eval time =   22140.04 ms /   120 runs   (  184.50 ms per token,     5.42 tokens per second)\n",
      "llama_print_timings:       total time =   61199.54 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      11.27 ms /    43 runs   (    0.26 ms per token,  3816.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27346.08 ms /   154 tokens (  177.57 ms per token,     5.63 tokens per second)\n",
      "llama_print_timings:        eval time =    8871.52 ms /    42 runs   (  211.23 ms per token,     4.73 tokens per second)\n",
      "llama_print_timings:       total time =   36304.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      15.06 ms /    76 runs   (    0.20 ms per token,  5046.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =   25355.99 ms /   153 tokens (  165.73 ms per token,     6.03 tokens per second)\n",
      "llama_print_timings:        eval time =   11710.61 ms /    75 runs   (  156.14 ms per token,     6.40 tokens per second)\n",
      "llama_print_timings:       total time =   37189.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      13.28 ms /    49 runs   (    0.27 ms per token,  3688.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =   29711.04 ms /   195 tokens (  152.36 ms per token,     6.56 tokens per second)\n",
      "llama_print_timings:        eval time =   10182.89 ms /    48 runs   (  212.14 ms per token,     4.71 tokens per second)\n",
      "llama_print_timings:       total time =   39995.70 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      42.44 ms /   199 runs   (    0.21 ms per token,  4689.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   25780.24 ms /   148 tokens (  174.19 ms per token,     5.74 tokens per second)\n",
      "llama_print_timings:        eval time =   33196.78 ms /   198 runs   (  167.66 ms per token,     5.96 tokens per second)\n",
      "llama_print_timings:       total time =   59323.35 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       1.90 ms /     7 runs   (    0.27 ms per token,  3690.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17901.99 ms /   128 tokens (  139.86 ms per token,     7.15 tokens per second)\n",
      "llama_print_timings:        eval time =    1281.58 ms /     6 runs   (  213.60 ms per token,     4.68 tokens per second)\n",
      "llama_print_timings:       total time =   19199.04 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      24.52 ms /    87 runs   (    0.28 ms per token,  3547.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27727.02 ms /   156 tokens (  177.74 ms per token,     5.63 tokens per second)\n",
      "llama_print_timings:        eval time =   18198.61 ms /    86 runs   (  211.61 ms per token,     4.73 tokens per second)\n",
      "llama_print_timings:       total time =   46112.89 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      24.30 ms /   130 runs   (    0.19 ms per token,  5350.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =   40645.61 ms /   251 tokens (  161.93 ms per token,     6.18 tokens per second)\n",
      "llama_print_timings:        eval time =   18782.08 ms /   129 runs   (  145.60 ms per token,     6.87 tokens per second)\n",
      "llama_print_timings:       total time =   59629.07 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       8.35 ms /    31 runs   (    0.27 ms per token,  3712.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =   46618.46 ms /   261 tokens (  178.61 ms per token,     5.60 tokens per second)\n",
      "llama_print_timings:        eval time =    6338.75 ms /    30 runs   (  211.29 ms per token,     4.73 tokens per second)\n",
      "llama_print_timings:       total time =   53021.16 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       2.02 ms /     7 runs   (    0.29 ms per token,  3461.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19105.40 ms /   109 tokens (  175.28 ms per token,     5.71 tokens per second)\n",
      "llama_print_timings:        eval time =    1232.58 ms /     6 runs   (  205.43 ms per token,     4.87 tokens per second)\n",
      "llama_print_timings:       total time =   20351.81 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      30.40 ms /   127 runs   (    0.24 ms per token,  4177.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   34196.40 ms /   240 tokens (  142.49 ms per token,     7.02 tokens per second)\n",
      "llama_print_timings:        eval time =   24140.22 ms /   126 runs   (  191.59 ms per token,     5.22 tokens per second)\n",
      "llama_print_timings:       total time =   58583.73 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       9.58 ms /    35 runs   (    0.27 ms per token,  3651.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19961.02 ms /   114 tokens (  175.10 ms per token,     5.71 tokens per second)\n",
      "llama_print_timings:        eval time =    7133.18 ms /    34 runs   (  209.80 ms per token,     4.77 tokens per second)\n",
      "llama_print_timings:       total time =   27166.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       2.69 ms /    10 runs   (    0.27 ms per token,  3710.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =   72042.35 ms /   461 tokens (  156.27 ms per token,     6.40 tokens per second)\n",
      "llama_print_timings:        eval time =    1884.92 ms /     9 runs   (  209.44 ms per token,     4.77 tokens per second)\n",
      "llama_print_timings:       total time =   73947.45 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      13.25 ms /    74 runs   (    0.18 ms per token,  5587.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =   58914.48 ms /   339 tokens (  173.79 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:        eval time =   10217.95 ms /    73 runs   (  139.97 ms per token,     7.14 tokens per second)\n",
      "llama_print_timings:       total time =   69236.87 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       8.97 ms /    35 runs   (    0.26 ms per token,  3900.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =   52893.34 ms /   325 tokens (  162.75 ms per token,     6.14 tokens per second)\n",
      "llama_print_timings:        eval time =    7256.13 ms /    34 runs   (  213.42 ms per token,     4.69 tokens per second)\n",
      "llama_print_timings:       total time =   60222.67 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       1.51 ms /     6 runs   (    0.25 ms per token,  3984.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21821.84 ms /   125 tokens (  174.57 ms per token,     5.73 tokens per second)\n",
      "llama_print_timings:        eval time =    1030.18 ms /     5 runs   (  206.04 ms per token,     4.85 tokens per second)\n",
      "llama_print_timings:       total time =   22864.98 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       9.32 ms /    52 runs   (    0.18 ms per token,  5579.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =   26144.12 ms /   189 tokens (  138.33 ms per token,     7.23 tokens per second)\n",
      "llama_print_timings:        eval time =    7287.54 ms /    51 runs   (  142.89 ms per token,     7.00 tokens per second)\n",
      "llama_print_timings:       total time =   33506.00 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      12.78 ms /    46 runs   (    0.28 ms per token,  3599.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =   30807.69 ms /   182 tokens (  169.27 ms per token,     5.91 tokens per second)\n",
      "llama_print_timings:        eval time =    9525.36 ms /    45 runs   (  211.67 ms per token,     4.72 tokens per second)\n",
      "llama_print_timings:       total time =   40429.20 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       1.98 ms /     8 runs   (    0.25 ms per token,  4044.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18651.69 ms /   106 tokens (  175.96 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:        eval time =    1460.85 ms /     7 runs   (  208.69 ms per token,     4.79 tokens per second)\n",
      "llama_print_timings:       total time =   20128.51 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      18.48 ms /    70 runs   (    0.26 ms per token,  3787.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =   49189.08 ms /   320 tokens (  153.72 ms per token,     6.51 tokens per second)\n",
      "llama_print_timings:        eval time =   13727.75 ms /    69 runs   (  198.95 ms per token,     5.03 tokens per second)\n",
      "llama_print_timings:       total time =   63053.41 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      10.04 ms /    36 runs   (    0.28 ms per token,  3587.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =   20468.29 ms /   116 tokens (  176.45 ms per token,     5.67 tokens per second)\n",
      "llama_print_timings:        eval time =    7413.44 ms /    35 runs   (  211.81 ms per token,     4.72 tokens per second)\n",
      "llama_print_timings:       total time =   27965.96 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      24.09 ms /   128 runs   (    0.19 ms per token,  5312.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =   43498.57 ms /   248 tokens (  175.40 ms per token,     5.70 tokens per second)\n",
      "llama_print_timings:        eval time =   18288.65 ms /   127 runs   (  144.01 ms per token,     6.94 tokens per second)\n",
      "llama_print_timings:       total time =   61981.64 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       1.66 ms /     6 runs   (    0.28 ms per token,  3616.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27940.87 ms /   166 tokens (  168.32 ms per token,     5.94 tokens per second)\n",
      "llama_print_timings:        eval time =    1056.93 ms /     5 runs   (  211.39 ms per token,     4.73 tokens per second)\n",
      "llama_print_timings:       total time =   29011.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      38.36 ms /   147 runs   (    0.26 ms per token,  3832.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =   26754.56 ms /   152 tokens (  176.02 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:        eval time =   29543.42 ms /   146 runs   (  202.35 ms per token,     4.94 tokens per second)\n",
      "llama_print_timings:       total time =   56602.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      21.14 ms /    84 runs   (    0.25 ms per token,  3973.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =   52008.31 ms /   313 tokens (  166.16 ms per token,     6.02 tokens per second)\n",
      "llama_print_timings:        eval time =   16340.34 ms /    83 runs   (  196.87 ms per token,     5.08 tokens per second)\n",
      "llama_print_timings:       total time =   68513.12 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       8.76 ms /    31 runs   (    0.28 ms per token,  3537.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27094.92 ms /   183 tokens (  148.06 ms per token,     6.75 tokens per second)\n",
      "llama_print_timings:        eval time =    6376.09 ms /    30 runs   (  212.54 ms per token,     4.71 tokens per second)\n",
      "llama_print_timings:       total time =   33536.60 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      20.00 ms /    73 runs   (    0.27 ms per token,  3650.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =   30829.41 ms /   175 tokens (  176.17 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:        eval time =   15247.43 ms /    72 runs   (  211.77 ms per token,     4.72 tokens per second)\n",
      "llama_print_timings:       total time =   46232.35 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       1.40 ms /     8 runs   (    0.18 ms per token,  5702.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =   23531.17 ms /   179 tokens (  131.46 ms per token,     7.61 tokens per second)\n",
      "llama_print_timings:        eval time =     984.31 ms /     7 runs   (  140.62 ms per token,     7.11 tokens per second)\n",
      "llama_print_timings:       total time =   24527.52 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      17.30 ms /    64 runs   (    0.27 ms per token,  3700.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27014.32 ms /   153 tokens (  176.56 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:        eval time =   13309.82 ms /    63 runs   (  211.27 ms per token,     4.73 tokens per second)\n",
      "llama_print_timings:       total time =   40457.85 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      11.37 ms /    42 runs   (    0.27 ms per token,  3692.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18708.44 ms /   107 tokens (  174.85 ms per token,     5.72 tokens per second)\n",
      "llama_print_timings:        eval time =    8640.70 ms /    41 runs   (  210.75 ms per token,     4.74 tokens per second)\n",
      "llama_print_timings:       total time =   27440.33 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       2.02 ms /     8 runs   (    0.25 ms per token,  3968.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =   76040.76 ms /   460 tokens (  165.31 ms per token,     6.05 tokens per second)\n",
      "llama_print_timings:        eval time =    1445.19 ms /     7 runs   (  206.46 ms per token,     4.84 tokens per second)\n",
      "llama_print_timings:       total time =   77503.65 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       1.63 ms /    10 runs   (    0.16 ms per token,  6131.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27568.54 ms /   162 tokens (  170.18 ms per token,     5.88 tokens per second)\n",
      "llama_print_timings:        eval time =    1200.42 ms /     9 runs   (  133.38 ms per token,     7.50 tokens per second)\n",
      "llama_print_timings:       total time =   28782.53 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       6.45 ms /    25 runs   (    0.26 ms per token,  3877.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =   23347.26 ms /   172 tokens (  135.74 ms per token,     7.37 tokens per second)\n",
      "llama_print_timings:        eval time =    5028.92 ms /    24 runs   (  209.54 ms per token,     4.77 tokens per second)\n",
      "llama_print_timings:       total time =   28425.00 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       1.79 ms /     7 runs   (    0.26 ms per token,  3921.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17776.81 ms /   101 tokens (  176.01 ms per token,     5.68 tokens per second)\n",
      "llama_print_timings:        eval time =    1237.33 ms /     6 runs   (  206.22 ms per token,     4.85 tokens per second)\n",
      "llama_print_timings:       total time =   19027.45 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       7.30 ms /    38 runs   (    0.19 ms per token,  5205.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =   62435.65 ms /   382 tokens (  163.44 ms per token,     6.12 tokens per second)\n",
      "llama_print_timings:        eval time =    5286.40 ms /    37 runs   (  142.88 ms per token,     7.00 tokens per second)\n",
      "llama_print_timings:       total time =   67776.23 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       1.90 ms /     7 runs   (    0.27 ms per token,  3682.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   26232.00 ms /   161 tokens (  162.93 ms per token,     6.14 tokens per second)\n",
      "llama_print_timings:        eval time =    1255.53 ms /     6 runs   (  209.25 ms per token,     4.78 tokens per second)\n",
      "llama_print_timings:       total time =   27503.99 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       7.67 ms /    27 runs   (    0.28 ms per token,  3521.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =   31958.02 ms /   181 tokens (  176.56 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:        eval time =    5488.18 ms /    26 runs   (  211.08 ms per token,     4.74 tokens per second)\n",
      "llama_print_timings:       total time =   37502.71 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      23.66 ms /    89 runs   (    0.27 ms per token,  3761.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =   61132.14 ms /   382 tokens (  160.03 ms per token,     6.25 tokens per second)\n",
      "llama_print_timings:        eval time =   18618.92 ms /    88 runs   (  211.58 ms per token,     4.73 tokens per second)\n",
      "llama_print_timings:       total time =   79947.30 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       4.04 ms /    18 runs   (    0.22 ms per token,  4454.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =   23990.65 ms /   138 tokens (  173.85 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:        eval time =    3135.08 ms /    17 runs   (  184.42 ms per token,     5.42 tokens per second)\n",
      "llama_print_timings:       total time =   27157.72 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      23.42 ms /    84 runs   (    0.28 ms per token,  3586.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =   48686.49 ms /   310 tokens (  157.05 ms per token,     6.37 tokens per second)\n",
      "llama_print_timings:        eval time =   17587.90 ms /    83 runs   (  211.90 ms per token,     4.72 tokens per second)\n",
      "llama_print_timings:       total time =   66460.90 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      10.51 ms /    57 runs   (    0.18 ms per token,  5422.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =   38573.64 ms /   222 tokens (  173.76 ms per token,     5.76 tokens per second)\n",
      "llama_print_timings:        eval time =    8140.76 ms /    56 runs   (  145.37 ms per token,     6.88 tokens per second)\n",
      "llama_print_timings:       total time =   46802.14 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       8.97 ms /    33 runs   (    0.27 ms per token,  3680.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =   43347.36 ms /   287 tokens (  151.04 ms per token,     6.62 tokens per second)\n",
      "llama_print_timings:        eval time =    6785.19 ms /    32 runs   (  212.04 ms per token,     4.72 tokens per second)\n",
      "llama_print_timings:       total time =   50200.89 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      11.84 ms /    46 runs   (    0.26 ms per token,  3884.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =   48652.43 ms /   272 tokens (  178.87 ms per token,     5.59 tokens per second)\n",
      "llama_print_timings:        eval time =    9442.31 ms /    45 runs   (  209.83 ms per token,     4.77 tokens per second)\n",
      "llama_print_timings:       total time =   58190.14 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       8.93 ms /    38 runs   (    0.24 ms per token,  4253.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21326.67 ms /   149 tokens (  143.13 ms per token,     6.99 tokens per second)\n",
      "llama_print_timings:        eval time =    6491.73 ms /    37 runs   (  175.45 ms per token,     5.70 tokens per second)\n",
      "llama_print_timings:       total time =   27888.49 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       6.19 ms /    23 runs   (    0.27 ms per token,  3715.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =   57536.67 ms /   323 tokens (  178.13 ms per token,     5.61 tokens per second)\n",
      "llama_print_timings:        eval time =    4650.25 ms /    22 runs   (  211.38 ms per token,     4.73 tokens per second)\n",
      "llama_print_timings:       total time =   62237.73 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      10.71 ms /    39 runs   (    0.27 ms per token,  3641.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =   32637.06 ms /   243 tokens (  134.31 ms per token,     7.45 tokens per second)\n",
      "llama_print_timings:        eval time =    8162.09 ms /    38 runs   (  214.79 ms per token,     4.66 tokens per second)\n",
      "llama_print_timings:       total time =   40882.09 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       2.04 ms /     8 runs   (    0.25 ms per token,  3925.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =   22984.89 ms /   130 tokens (  176.81 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:        eval time =    1464.53 ms /     7 runs   (  209.22 ms per token,     4.78 tokens per second)\n",
      "llama_print_timings:       total time =   24465.29 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       9.34 ms /    34 runs   (    0.27 ms per token,  3639.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18719.17 ms /   107 tokens (  174.95 ms per token,     5.72 tokens per second)\n",
      "llama_print_timings:        eval time =    6907.91 ms /    33 runs   (  209.33 ms per token,     4.78 tokens per second)\n",
      "llama_print_timings:       total time =   25701.13 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       9.68 ms /    50 runs   (    0.19 ms per token,  5165.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =   31176.06 ms /   193 tokens (  161.53 ms per token,     6.19 tokens per second)\n",
      "llama_print_timings:        eval time =    7493.50 ms /    49 runs   (  152.93 ms per token,     6.54 tokens per second)\n",
      "llama_print_timings:       total time =   38748.14 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      31.71 ms /   134 runs   (    0.24 ms per token,  4226.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =   56222.91 ms /   325 tokens (  172.99 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:        eval time =   24429.52 ms /   133 runs   (  183.68 ms per token,     5.44 tokens per second)\n",
      "llama_print_timings:       total time =   80910.48 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      13.48 ms /    50 runs   (    0.27 ms per token,  3708.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15116.30 ms /   114 tokens (  132.60 ms per token,     7.54 tokens per second)\n",
      "llama_print_timings:        eval time =   10438.71 ms /    49 runs   (  213.03 ms per token,     4.69 tokens per second)\n",
      "llama_print_timings:       total time =   25660.09 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      22.33 ms /    77 runs   (    0.29 ms per token,  3447.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =   28729.75 ms /   164 tokens (  175.18 ms per token,     5.71 tokens per second)\n",
      "llama_print_timings:        eval time =   16089.62 ms /    76 runs   (  211.71 ms per token,     4.72 tokens per second)\n",
      "llama_print_timings:       total time =   44990.56 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       5.21 ms /    19 runs   (    0.27 ms per token,  3643.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17503.63 ms /   100 tokens (  175.04 ms per token,     5.71 tokens per second)\n",
      "llama_print_timings:        eval time =    3745.80 ms /    18 runs   (  208.10 ms per token,     4.81 tokens per second)\n",
      "llama_print_timings:       total time =   21289.87 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       9.61 ms /    36 runs   (    0.27 ms per token,  3744.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =   42022.47 ms /   297 tokens (  141.49 ms per token,     7.07 tokens per second)\n",
      "llama_print_timings:        eval time =    7505.85 ms /    35 runs   (  214.45 ms per token,     4.66 tokens per second)\n",
      "llama_print_timings:       total time =   49608.32 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      20.78 ms /    81 runs   (    0.26 ms per token,  3898.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =   50344.33 ms /   282 tokens (  178.53 ms per token,     5.60 tokens per second)\n",
      "llama_print_timings:        eval time =   16304.89 ms /    80 runs   (  203.81 ms per token,     4.91 tokens per second)\n",
      "llama_print_timings:       total time =   66817.74 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      14.23 ms /    52 runs   (    0.27 ms per token,  3653.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47795.51 ms /   304 tokens (  157.22 ms per token,     6.36 tokens per second)\n",
      "llama_print_timings:        eval time =   10792.66 ms /    51 runs   (  211.62 ms per token,     4.73 tokens per second)\n",
      "llama_print_timings:       total time =   58699.56 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       9.95 ms /    41 runs   (    0.24 ms per token,  4121.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =   40405.94 ms /   233 tokens (  173.42 ms per token,     5.77 tokens per second)\n",
      "llama_print_timings:        eval time =    7412.00 ms /    40 runs   (  185.30 ms per token,     5.40 tokens per second)\n",
      "llama_print_timings:       total time =   47892.92 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       7.01 ms /    25 runs   (    0.28 ms per token,  3566.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18110.66 ms /   134 tokens (  135.15 ms per token,     7.40 tokens per second)\n",
      "llama_print_timings:        eval time =    5071.76 ms /    24 runs   (  211.32 ms per token,     4.73 tokens per second)\n",
      "llama_print_timings:       total time =   23235.64 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       7.78 ms /    28 runs   (    0.28 ms per token,  3600.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =   42935.54 ms /   243 tokens (  176.69 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:        eval time =    5698.18 ms /    27 runs   (  211.04 ms per token,     4.74 tokens per second)\n",
      "llama_print_timings:       total time =   48691.10 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      13.49 ms /    74 runs   (    0.18 ms per token,  5483.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19653.06 ms /   127 tokens (  154.75 ms per token,     6.46 tokens per second)\n",
      "llama_print_timings:        eval time =   10480.74 ms /    73 runs   (  143.57 ms per token,     6.97 tokens per second)\n",
      "llama_print_timings:       total time =   30244.75 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      13.42 ms /    51 runs   (    0.26 ms per token,  3800.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =   56209.82 ms /   315 tokens (  178.44 ms per token,     5.60 tokens per second)\n",
      "llama_print_timings:        eval time =   10595.26 ms /    50 runs   (  211.91 ms per token,     4.72 tokens per second)\n",
      "llama_print_timings:       total time =   66911.61 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      20.99 ms /    87 runs   (    0.24 ms per token,  4144.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   25026.84 ms /   178 tokens (  140.60 ms per token,     7.11 tokens per second)\n",
      "llama_print_timings:        eval time =   15916.27 ms /    86 runs   (  185.07 ms per token,     5.40 tokens per second)\n",
      "llama_print_timings:       total time =   41117.75 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      25.69 ms /   102 runs   (    0.25 ms per token,  3971.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =   49927.50 ms /   282 tokens (  177.05 ms per token,     5.65 tokens per second)\n",
      "llama_print_timings:        eval time =   19506.80 ms /   101 runs   (  193.14 ms per token,     5.18 tokens per second)\n",
      "llama_print_timings:       total time =   69634.91 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      15.19 ms /    56 runs   (    0.27 ms per token,  3687.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =   52977.45 ms /   329 tokens (  161.03 ms per token,     6.21 tokens per second)\n",
      "llama_print_timings:        eval time =   11637.30 ms /    55 runs   (  211.59 ms per token,     4.73 tokens per second)\n",
      "llama_print_timings:       total time =   64732.98 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      11.39 ms /    45 runs   (    0.25 ms per token,  3950.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =   51918.30 ms /   341 tokens (  152.25 ms per token,     6.57 tokens per second)\n",
      "llama_print_timings:        eval time =    8727.10 ms /    44 runs   (  198.34 ms per token,     5.04 tokens per second)\n",
      "llama_print_timings:       total time =   60732.49 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      13.02 ms /    49 runs   (    0.27 ms per token,  3762.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =   38312.69 ms /   216 tokens (  177.37 ms per token,     5.64 tokens per second)\n",
      "llama_print_timings:        eval time =   10123.14 ms /    48 runs   (  210.90 ms per token,     4.74 tokens per second)\n",
      "llama_print_timings:       total time =   48542.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      23.37 ms /    98 runs   (    0.24 ms per token,  4193.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45008.75 ms /   299 tokens (  150.53 ms per token,     6.64 tokens per second)\n",
      "llama_print_timings:        eval time =   18741.36 ms /    97 runs   (  193.21 ms per token,     5.18 tokens per second)\n",
      "llama_print_timings:       total time =   63945.28 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       1.97 ms /     8 runs   (    0.25 ms per token,  4071.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =   31489.49 ms /   181 tokens (  173.98 ms per token,     5.75 tokens per second)\n",
      "llama_print_timings:        eval time =    1461.12 ms /     7 runs   (  208.73 ms per token,     4.79 tokens per second)\n",
      "llama_print_timings:       total time =   32966.45 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       5.85 ms /    28 runs   (    0.21 ms per token,  4784.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   36878.34 ms /   218 tokens (  169.17 ms per token,     5.91 tokens per second)\n",
      "llama_print_timings:        eval time =    4288.80 ms /    27 runs   (  158.84 ms per token,     6.30 tokens per second)\n",
      "llama_print_timings:       total time =   41215.44 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       1.12 ms /     6 runs   (    0.19 ms per token,  5371.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18147.82 ms /   141 tokens (  128.71 ms per token,     7.77 tokens per second)\n",
      "llama_print_timings:        eval time =     716.00 ms /     5 runs   (  143.20 ms per token,     6.98 tokens per second)\n",
      "llama_print_timings:       total time =   18873.22 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       2.81 ms /    12 runs   (    0.23 ms per token,  4265.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =   82064.73 ms /   458 tokens (  179.18 ms per token,     5.58 tokens per second)\n",
      "llama_print_timings:        eval time =    2049.87 ms /    11 runs   (  186.35 ms per token,     5.37 tokens per second)\n",
      "llama_print_timings:       total time =   84135.66 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       1.23 ms /     7 runs   (    0.18 ms per token,  5686.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =   22005.86 ms /   168 tokens (  130.99 ms per token,     7.63 tokens per second)\n",
      "llama_print_timings:        eval time =     827.63 ms /     6 runs   (  137.94 ms per token,     7.25 tokens per second)\n",
      "llama_print_timings:       total time =   22843.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       4.03 ms /    15 runs   (    0.27 ms per token,  3722.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =   33126.42 ms /   194 tokens (  170.75 ms per token,     5.86 tokens per second)\n",
      "llama_print_timings:        eval time =    2940.66 ms /    14 runs   (  210.05 ms per token,     4.76 tokens per second)\n",
      "llama_print_timings:       total time =   36096.94 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       1.89 ms /     7 runs   (    0.27 ms per token,  3705.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21819.92 ms /   125 tokens (  174.56 ms per token,     5.73 tokens per second)\n",
      "llama_print_timings:        eval time =    1252.63 ms /     6 runs   (  208.77 ms per token,     4.79 tokens per second)\n",
      "llama_print_timings:       total time =   23086.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       5.50 ms /    31 runs   (    0.18 ms per token,  5632.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   31866.30 ms /   204 tokens (  156.21 ms per token,     6.40 tokens per second)\n",
      "llama_print_timings:        eval time =    4152.89 ms /    30 runs   (  138.43 ms per token,     7.22 tokens per second)\n",
      "llama_print_timings:       total time =   36063.74 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      15.48 ms /    57 runs   (    0.27 ms per token,  3683.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16534.40 ms /   114 tokens (  145.04 ms per token,     6.89 tokens per second)\n",
      "llama_print_timings:        eval time =   11894.96 ms /    56 runs   (  212.41 ms per token,     4.71 tokens per second)\n",
      "llama_print_timings:       total time =   28548.00 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =      12.08 ms /    46 runs   (    0.26 ms per token,  3806.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27109.51 ms /   155 tokens (  174.90 ms per token,     5.72 tokens per second)\n",
      "llama_print_timings:        eval time =    9508.79 ms /    45 runs   (  211.31 ms per token,     4.73 tokens per second)\n",
      "llama_print_timings:       total time =   36712.74 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   33122.91 ms\n",
      "llama_print_timings:      sample time =       2.92 ms /    11 runs   (    0.27 ms per token,  3772.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =   73904.10 ms /   459 tokens (  161.01 ms per token,     6.21 tokens per second)\n",
      "llama_print_timings:        eval time =    2088.25 ms /    10 runs   (  208.83 ms per token,     4.79 tokens per second)\n",
      "llama_print_timings:       total time =   76015.48 ms\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/llama2/llama-2-7b-chat.Q2_K.gguf\"\n",
    "model = Llama(model_path=model_path, chat_format=\"llama-2\")\n",
    "\n",
    "llm = llama2LLM(\"7b-Q2_K\", model)\n",
    "testing = TestingFramework(df_sample, llm)\n",
    "\n",
    "testing.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3cc6700-b5ca-4136-84b2-bb1e3940275c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>time</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>42.759319</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>19.459063</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>27.947745</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>40.444605</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>59.778341</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>84.137024</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       score        time   cost\n",
       "count  100.0  100.000000  100.0\n",
       "mean     0.0   42.759319    0.0\n",
       "std      0.0   19.459063    0.0\n",
       "min      0.0    0.000705    0.0\n",
       "25%      0.0   27.947745    0.0\n",
       "50%      0.0   40.444605    0.0\n",
       "75%      0.0   59.778341    0.0\n",
       "max      0.0   84.137024    0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores = testing.get_scores()\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25a9197-9c06-441f-bb1a-f954812b5f88",
   "metadata": {},
   "source": [
    "The time taken is very large, and the results are exceedingly bad. What is the reason for this? Let's take a look at the row output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7931ffb4-f8c0-4e72-8710-e2aba5008080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3599      Sure! Here are the numerical values for the ...\n",
       "10        Sure! According to the information provided ...\n",
       "3410      The numerical value for the percentage of ch...\n",
       "521       The percentage change in the allowance for d...\n",
       "2679      Sure! The percent of growth in total net rev...\n",
       "                              ...                        \n",
       "1310                                                -$9.3\n",
       "1711      The 2010 balance for securities purchased un...\n",
       "3567      Sure! The percentage change in Entergy New O...\n",
       "1431      Sure! The growth rate of debt to capital rat...\n",
       "2867      The percentage change in total operating exp...\n",
       "Name: row_output, Length: 100, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing.data[\"row_output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aaf6cc-e2c4-41d2-8bfd-ed7fc401e059",
   "metadata": {},
   "source": [
    "We see that the LLM does not follow the instruction of outputting just a number. Moreover, a deeper analysis reveals that the computations described in the output are usually incomplete, so using another LLM to simply extract the numerical result from the text is not an option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21b862b-db54-4054-b254-ba517fdad971",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a53f56-3339-4099-beb8-87ce47f7ec32",
   "metadata": {},
   "source": [
    "In this section we consider OpenAI's models, expected to be better (both in terms of accuracy and time elapsed), though they required paying.\n",
    "\n",
    "While the code below has been generated using my personal OpenAI API key, I am not sharing it in this code. To run the code, you will need your own API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "528f67a2-cd8b-4d84-9121-b89e74761c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = # your API here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8021c18e-3626-4974-bb23-bf1fa2743c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class openaiLLM:\n",
    "\n",
    "    def __init__(self, name, client, model, prices=[0, 0]):\n",
    "        self.name = name\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.price_1kprompt, self.price_1kcompletion = prices\n",
    "\n",
    "    def predict(self, x):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        output, cost = self.create_chat_completion(x)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        return output, elapsed_time, cost\n",
    "\n",
    "    # UTILITIES\n",
    "\n",
    "    def create_chat_completion(self, x):\n",
    "\n",
    "        instructions = \"Answer the user's question returning only the numerical value: do not provide any textual comments.\"\n",
    "            \n",
    "        question = f\"QUESTION: {x['question']}\"\n",
    "        context = f\"INFORMATION:\\n'''\\n{x['pre_text']}\\n\\nTABLE:\\n{x['table']}\\n\\n{x['post_text']}\\n'''\"\n",
    "\n",
    "        query = question + \"\\n\\n\" + context\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            temperature=0,\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": instructions\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \"content\": query\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        output = response.choices[0].message.content\n",
    "        cost = (self.price_1kprompt * response.usage.prompt_tokens + self.price_1kcompletion * response.usage.completion_tokens)/1000\n",
    "\n",
    "        return output, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2579911d-134c-4615-b6fe-6a3b9f4d77b2",
   "metadata": {},
   "source": [
    "### GPT-3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ffb61f-ad95-4d23-959a-5a0d3d9b13b6",
   "metadata": {},
   "source": [
    "Let's start with the cheaper model, GPT-3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6d6b94c-8aa2-4a33-960c-e130af62f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI()\n",
    "model = \"gpt-3.5-turbo-1106\"\n",
    "prices = [0.001, 0.002]\n",
    "\n",
    "llm = openaiLLM(\"gpt-3.5\", client, model, prices)\n",
    "testing = TestingFramework(df_sample, llm)\n",
    "\n",
    "testing.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79685cf4-6da2-4b4b-8e2b-25081dfed77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>time</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.906318</td>\n",
       "      <td>0.000982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.219043</td>\n",
       "      <td>0.357965</td>\n",
       "      <td>0.000388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408353</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.614724</td>\n",
       "      <td>0.000769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.819332</td>\n",
       "      <td>0.000965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.024740</td>\n",
       "      <td>0.001149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.253693</td>\n",
       "      <td>0.002858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            score        time        cost\n",
       "count  100.000000  100.000000  100.000000\n",
       "mean     0.050000    0.906318    0.000982\n",
       "std      0.219043    0.357965    0.000388\n",
       "min      0.000000    0.408353    0.000261\n",
       "25%      0.000000    0.614724    0.000769\n",
       "50%      0.000000    0.819332    0.000965\n",
       "75%      0.000000    1.024740    0.001149\n",
       "max      1.000000    2.253693    0.002858"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores = testing.get_scores()\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27779115-9f8c-493d-b732-52f2f1a0ef06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time:  90.631844997406\n",
      "total cost:  0.098215\n"
     ]
    }
   ],
   "source": [
    "print(\"total time: \", df_scores[\"time\"][\"count\"] * df_scores[\"time\"][\"mean\"])\n",
    "print(\"total cost: \", df_scores[\"cost\"][\"count\"] * df_scores[\"cost\"][\"mean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad94824-ab45-4ff2-9dc0-1b5760f75030",
   "metadata": {},
   "source": [
    "We see that only 5% of the questions are answered correctly. Let's see how the situation changes if we adopt a more advanced model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e39cf3-24a4-4736-934e-28916ce14afb",
   "metadata": {},
   "source": [
    "### GPT-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1eb09e-b1f2-4186-9a41-991798bebc17",
   "metadata": {},
   "source": [
    "GPT-4 is currently the most powerful of OpenAI's models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "785c871f-4d36-4788-ad3d-e26aea2d994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI()\n",
    "model = \"gpt-4-1106-preview\"\n",
    "prices = [0.01, 0.03]\n",
    "\n",
    "llm = openaiLLM(\"gpt-4\", client, model, prices)\n",
    "testing = TestingFramework(df_sample, llm)\n",
    "\n",
    "testing.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48b84c28-c638-43ff-a36f-34d355123939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>time</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.998892</td>\n",
       "      <td>0.009829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.490207</td>\n",
       "      <td>0.430136</td>\n",
       "      <td>0.003875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.488925</td>\n",
       "      <td>0.002640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.817771</td>\n",
       "      <td>0.007698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.908447</td>\n",
       "      <td>0.009730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.024689</td>\n",
       "      <td>0.011545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.071766</td>\n",
       "      <td>0.028710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            score        time        cost\n",
       "count  100.000000  100.000000  100.000000\n",
       "mean     0.390000    0.998892    0.009829\n",
       "std      0.490207    0.430136    0.003875\n",
       "min      0.000000    0.488925    0.002640\n",
       "25%      0.000000    0.817771    0.007698\n",
       "50%      0.000000    0.908447    0.009730\n",
       "75%      1.000000    1.024689    0.011545\n",
       "max      1.000000    3.071766    0.028710"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores = testing.get_scores()\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f53b97a-1b02-4b4f-9955-39513468da72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time:  99.88923597335815\n",
      "total cost:  0.98286\n"
     ]
    }
   ],
   "source": [
    "print(\"total time: \", df_scores[\"time\"][\"count\"] * df_scores[\"time\"][\"mean\"])\n",
    "print(\"total cost: \", df_scores[\"cost\"][\"count\"] * df_scores[\"cost\"][\"mean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ae9dd7-30a2-444e-99a5-edd917b05342",
   "metadata": {},
   "source": [
    "As expected, the situation got much better, with 39% of questions answered correctly, though the price is also gone up.\n",
    "\n",
    "Let's take a look at the row output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6386ae64-7122-4669-b187-2f0c5d629ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3599        341\n",
       "10      $45,161\n",
       "3410      1.25%\n",
       "521        16.5\n",
       "2679       77.8\n",
       "         ...   \n",
       "1310     -88.57\n",
       "1711    202,002\n",
       "3567    -10.01%\n",
       "1431       5.3%\n",
       "2867     178.43\n",
       "Name: row_output, Length: 100, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing.data[\"row_output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371bdd3d-16cf-4ce5-bb73-439b2b79d81d",
   "metadata": {},
   "source": [
    "We see that now the instruction of returning only numbers is satisfied. Upon a deeper inspection, it turns out that most of the wrong cases are due to either incomplete computations or arbitrary (and wrong) approximations of the correct output. This is expected, as LLM are known to not be great at mathematical tasks. To improve on this, we can make use of a chain of prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d51c79-7808-49a1-a916-68884f680a3f",
   "metadata": {},
   "source": [
    "# OpenAI with Chain of Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c78b040-24e3-4261-8e24-45a7c7921252",
   "metadata": {},
   "outputs": [],
   "source": [
    "class openai_chainLLM:\n",
    "\n",
    "    def __init__(self, name, client, model, prices=[0, 0]):\n",
    "        self.name = name\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.price_1kprompt, self.price_1kcompletion = prices\n",
    "\n",
    "    def predict(self, x):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        output, cost = self.create_chat_completion(x)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        return output, elapsed_time, cost\n",
    "\n",
    "    # UTILITIES\n",
    "\n",
    "    def create_chat_completion(self, x):\n",
    "\n",
    "        # first step\n",
    "\n",
    "        instructions = \"Answer the user's question using the information provided. Present a step-by-step reasoning.\"\n",
    "        question = f\"QUESTION: {x['question']}\"\n",
    "        context = f\"INFORMATION:\\n'''\\n{x['pre_text']}\\n\\nTABLE:\\n{x['table']}\\n\\n{x['post_text']}\\n'''\"\n",
    "        query = question + \"\\n\\n\" + context\n",
    "        output1, cost1 = self.api_call(instructions, query)\n",
    "        \n",
    "        # second step\n",
    "        \n",
    "        instructions = \"Answer the user's question from the context provided. Return only the numerical value: no textual comments.\"    \n",
    "        question = f\"QUESTION: {x['question']}\"\n",
    "        context = f\"CONTEXT:\\n'''\\n{output1}\\n'''\"\n",
    "        query = question + \"\\n\\n\" + context\n",
    "        output2, cost2 = self.api_call(instructions, query)\n",
    "\n",
    "        cost = cost1 + cost2\n",
    "\n",
    "        return output2, cost\n",
    "\n",
    "    def api_call(self, instructions, query):\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            temperature=0,\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": instructions\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \"content\": query\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        output = response.choices[0].message.content\n",
    "        cost = (self.price_1kprompt * response.usage.prompt_tokens + self.price_1kcompletion * response.usage.completion_tokens)/1000\n",
    "\n",
    "        return output, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "acf07c9f-de78-4849-ab74-5198cd22a123",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI()\n",
    "model = \"gpt-4-1106-preview\"\n",
    "prices = [0.01, 0.03]\n",
    "\n",
    "llm = openai_chainLLM(\"gpt-4\", client, model, prices)\n",
    "testing = TestingFramework(df_sample, llm)\n",
    "\n",
    "testing.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54d71a00-a9ac-423f-bf27-8c0a67aed3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>time</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.480000</td>\n",
       "      <td>18.182599</td>\n",
       "      <td>0.020415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.502117</td>\n",
       "      <td>8.070635</td>\n",
       "      <td>0.004419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.024198</td>\n",
       "      <td>0.010610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.780622</td>\n",
       "      <td>0.017225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.213748</td>\n",
       "      <td>0.020255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.164210</td>\n",
       "      <td>0.022225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>41.637337</td>\n",
       "      <td>0.042540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            score        time        cost\n",
       "count  100.000000  100.000000  100.000000\n",
       "mean     0.480000   18.182599    0.020415\n",
       "std      0.502117    8.070635    0.004419\n",
       "min      0.000000    7.024198    0.010610\n",
       "25%      0.000000   11.780622    0.017225\n",
       "50%      0.000000   16.213748    0.020255\n",
       "75%      1.000000   23.164210    0.022225\n",
       "max      1.000000   41.637337    0.042540"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores = testing.get_scores()\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51c0df3b-6fd2-433e-91e1-3c6454cf2ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time:  1818.2598712444308\n",
      "total cost:  2.0415\n"
     ]
    }
   ],
   "source": [
    "print(\"total time: \", df_scores[\"time\"][\"count\"] * df_scores[\"time\"][\"mean\"])\n",
    "print(\"total cost: \", df_scores[\"cost\"][\"count\"] * df_scores[\"cost\"][\"mean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aead194c-8aa4-4e1e-a635-3f6e490b0e6c",
   "metadata": {},
   "source": [
    "While the price and the time required for a single answer went up (as expected), we see an improvement in the percentage of correct answers, which is now 48%. This indicates that the chain-of-prompt approach is potentially a good way to go. Further improvment (in terms of lowering cost) could be achieved by implementing RAG as a pre-selection of the information to be passed in the context for the first prompt (e.g., we might find that, for a question, the table is enough and thus we might decide not present the `pre_text` and the `post_text` to the LLM). However, given that these texts are not excessively long, we believe that not much would be gained in this specific case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
